#!/usr/bin/env python3

# Determine precision by assuming the operands are precise down to the unit.

from argparse import ArgumentParser
from fractions import Fraction
from itertools import count
import math
import sys

ap = ArgumentParser()
ap.add_argument('a', type=int, help='Numerator')
ap.add_argument('b', type=int, help='Denominator')


def roundG(x, precision):
    """Convert x to a string.

    precision is the number of significant digits.
    """
    return '{:.{}g}'.format(x, precision)


def enoughDigits(a, b, q):
    """Show only enough digits to be unique to the numerator."""
    for precision in count(1):
        qString = roundG(q, precision)
        if a == round(float(qString) * b):
            return qString


def loDenominator(a, b, q):
    """Show the simplest fraction that results in the right a when scaled up."""
    for denominator in count(1):
        numerator = round(denominator * q)
        if a == round(numerator * b / denominator):
            return f'{numerator}/{denominator}'


def main(argv):
    args = ap.parse_args()

    a = args.a
    b = args.b
    q = a / b

    print(
        roundG(
            q,
            math.ceil(math.log10(q)) - math.floor(math.log10(q / min(a, b))),
        ),
    )
    print(Fraction(a, b))
    print(enoughDigits(a, b, q))
    print(loDenominator(a, b, q))


if __name__ == '__main__':
    main(sys.argv[1:])
